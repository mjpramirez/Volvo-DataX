{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Model A.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"BJT7INz8R2UL","colab_type":"text"},"source":["# **MODEL A: YOLOv3 + SORT + ST-DenseNet** \n","## A unified framework for pedestrian intention prediction.\n","1. **YOLOv3** -> Object detector: responsible to identify and detect objects of interest in a given frame or image.\n","2. **SORT** -> Object Tracker: responsible to track the identified pedestrians for the sequence of frames and maintain unique IDs for each pedestrian.\n","\n","3. **Spatio-Temporal DenseNet** -> Classifier: responsible to classify every identified and tracked pedestrian's intention by using the last 16 frames of a pedetrian."]},{"cell_type":"markdown","metadata":{"id":"OsiF9o0QPVnI","colab_type":"text"},"source":["## **INSTRUCTIONS TO RUN THE MODEL**\n","\n","This project was completely developed on Google Colab.\n","\n","1. Clone the repository to Colab.\n","\n","2. Connect to GPU for better performance.\n","\n","3. Next click this link and download the files: https://drive.google.com/open?id=1HxKtxBva3US2AJfohlKfjYSdhHvjt2Yc\n","\n","4. From the downloaded folder **\"datax_volvo_additional_files\"**, open the folder YOLO and transfer the 3 files to one of the cloned folder named **\"checkpoints\"**.\n","\n","5. To run the remaining cells observe the comments and run appropriately. \n","\n","6. After running the run_model() function expect around 5 mins for GPU and 15 mins for CPU\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"yAkpG3EGkXGo","colab_type":"code","colab":{}},"source":["# run this to clone the repository Volvo-DataX\n","!git clone https://github.com/mjpramirez/Volvo-DataX"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bgNu6gB7NspT","colab_type":"code","colab":{}},"source":["# use this if cloning to google colab and to open notebooks and to access folders from the drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DGgcEDqHNyUl","colab_type":"code","colab":{}},"source":["# next run this\n","try:\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","\n","import tensorflow as tf\n","tf.__version__\n","import glob\n","import os\n","\n","!pip install filterpy\n","%cd Volvo-DataX"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wdx2AMWFXlHj","colab_type":"code","colab":{}},"source":["# Run this only if you are using GPU\n","!pip install -r requirements-gpu.txt\n","physical_devices = tf.config.experimental.list_physical_devices('GPU')\n","tf.config.experimental.set_memory_growth(physical_devices[0], True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MEtYy4i9Xm72","colab_type":"code","colab":{}},"source":["# Run this only if you are using CPU\n","!pip install -r requirements-ngpu.txt"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nRVfKbwcN-O3","colab_type":"code","colab":{}},"source":["# Run this to initialize the required components [YOLO, SORT and DenseNet]\n","import sys\n","from absl import app, logging, flags\n","from absl.flags import FLAGS\n","import time\n","import cv2\n","import numpy as np\n","import tensorflow as tf\n","from yolov3_tf2.models import (\n","    YoloV3, YoloV3Tiny\n",")\n","from yolov3_tf2.dataset import transform_images, load_tfrecord_dataset\n","from yolov3_tf2.utils import draw_outputs\n","\n","from sortn import *\n","\n","flags.DEFINE_string('classes', 'data/coco.names', 'path to classes file')\n","flags.DEFINE_string('weights', 'checkpoints/yolov3_train_5.tf','path to weights file')\n","flags.DEFINE_boolean('tiny', False, 'yolov3 or yolov3-tiny')\n","flags.DEFINE_integer('size', 416, 'resize images to')\n","flags.DEFINE_string('tfrecord', None, 'tfrecord instead of image')\n","flags.DEFINE_integer('num_classes', 1, 'number of classes in the model')\n","flags.DEFINE_string('video', 'data/JAAD_test_video_0339.mp4','path to video file or number for webcam)')\n","flags.DEFINE_string('output','Result_model_A.mp4', 'path to output video')\n","flags.DEFINE_string('output_format', 'mp4v', 'codec used in VideoWriter when saving video to file')\n","\n","app._run_init(['yolov3'], app.parse_flags_with_usage)\n","\n","#Reading the model from JSON file\n","with open('densenet_model.json', 'r') as json_file:\n","    json_savedModel= json_file.read()\n","\n","model_j = tf.keras.models.model_from_json(json_savedModel)\n","model_j.load_weights('1.hdf5')\n","\n","def pred_func(X_test):\n","  predictions = model_j.predict(X_test[0:1], verbose=0)\n","  Y = np.argmax(predictions[0], axis=0)\n","    \n","  return Y"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fKfCjninOfaz","colab_type":"code","colab":{}},"source":["# Set threshold\n","FLAGS.yolo_iou_threshold = 0.5\n","FLAGS.yolo_score_threshold = 0.5\n","\n","color = (255, 0, 0) \n","thickness = 2\n","\n","yolo = YoloV3(classes=FLAGS.num_classes)\n","\n","yolo.load_weights(FLAGS.weights).expect_partial()\n","logging.info('weights loaded')\n","\n","class_names = [c.strip() for c in open(FLAGS.classes).readlines()]\n","logging.info('classes loaded')\n","\n","\n","\n","def run_model():\n","  frame = 0\n","\n","  try:\n","      vid = cv2.VideoCapture(int(FLAGS.video))\n","  except:\n","      vid = cv2.VideoCapture(FLAGS.video)\n","\n","  out = None\n","\n","  if FLAGS.output:\n","      # by default VideoCapture returns float instead of int\n","      width = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH))\n","      height = int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","      fps = int(vid.get(cv2.CAP_PROP_FPS))\n","      codec = cv2.VideoWriter_fourcc(*FLAGS.output_format)\n","      out = cv2.VideoWriter(FLAGS.output, codec, fps, (width, height))\n","\n","  #create instance of SORT\n","  mot_tracker = Sort()\n","  rolling_data={}\n","\n","  while True:\n","      _, img = vid.read()\n","\n","      if img is None:\n","          break\n","      \n","      frame +=1\n","\n","      img_in = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n","      img_orig = np.copy(img_in)\n","      img_in = tf.expand_dims(img_in, 0)\n","      img_in = transform_images(img_in, FLAGS.size)\n","\n","\n","      boxes, scores, classes, nums = yolo.predict(img_in) # yolo prediction\n","      dets = boxes[:,:nums[0],:].reshape(nums[0], 4)  # filter pedestrians \n","      trackers = mot_tracker.update(dets[classes[0][:nums[0]] == 0]) # track the pedestrians\n","    \n","\n","      for d in trackers:\n","\n","        wh = np.flip(img.shape[0:2])    \n","        x1y1 = tuple((np.array(d[0:2]) * wh).astype(np.int32))\n","        x2y2 = tuple((np.array(d[2:4]) * wh).astype(np.int32))\n","\n","        y = 0\n","\n","        if int(d[4]) in list(rolling_data.keys()):\n","\n","          if len(rolling_data[int(d[4])]) == 16:\n","            \n","            seq = np.stack(np.array(rolling_data[int(d[4])]),axis=2) # (100*100*16*3)\n","            seq = np.expand_dims(seq, axis=0)\n","            y = pred_func(seq) # classification output\n","\n","          else:\n","\n","            seq = np.stack(np.array([rolling_data[int(d[4])][-1]] * 16),axis=2)\n","            seq = np.expand_dims(seq, axis=0)\n","            y = pred_func(seq) # classification output\n","\n","        # risky pedestrian identification thru box color\n","\n","        if y == 1:\n","          color = (0, 0, 255)\n","\n","        else:\n","          color = (0, 255, 0)\n","\n","        image = cv2.rectangle(img, x1y1, x2y2, color, thickness) \n","        image = cv2.putText(image, str(int(d[4])), org = (x1y1[0],x1y1[1]-5) , fontFace = cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=color, thickness=thickness)\n","        image = cv2.putText(image, \"Frame No: {}\".format(frame), (0, 30),cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255, 0, 0), 2)\n","\n","        # storing the data for last 16 frames\n","        try:\n","\n","          if int(d[4]) in list(rolling_data.keys()): # ID exists in dict\n","\n","            if len(rolling_data[int(d[4])]) < 16: # bboxes values for 16 frames\n","                \n","              cropped_seq = []\n","              cropped_img = cv2.resize(img_orig[x1y1[1]:x2y2[1], x1y1[0]:x2y2[0]],(100,100))\n","              rolling_data[int(d[4])].append(np.asarray(cropped_img)) # append the image      \n","\n","            else:\n","\n","              del rolling_data[int(d[4])][0] # delete oldest frame bbox and append latest frame bbox\n","              cropped_seq = []\n","              cropped_img = cv2.resize(img_orig[x1y1[1]:x2y2[1], x1y1[0]:x2y2[0]],(100,100))\n","              rolling_data[int(d[4])].append(np.asarray(cropped_img))\n","              \n","          else:\n","\n","            cropped_seq = []\n","            cropped_img = cv2.resize(img_orig[x1y1[1]:x2y2[1], x1y1[0]:x2y2[0]],(100,100))\n","            rolling_data[int(d[4])] = [np.asarray(cropped_img)]  \n","        except:\n","          pass \n","\n","\n","      if FLAGS.output:\n","\n","        out.write(img)\n","      #cv2.imshow('output', img)     \n","      if cv2.waitKey(1) == ord('q'):\n","        break\n","\n","  cv2.destroyAllWindows()\n","\n","  return"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vIX-tpSZi6zq","colab_type":"text"},"source":["### Run this to obtain the Model-A output as a video file named **'Result_model_A.mp4'**"]},{"cell_type":"code","metadata":{"id":"JuCF5_VKOujQ","colab_type":"code","colab":{}},"source":["run_model()"],"execution_count":0,"outputs":[]}]}