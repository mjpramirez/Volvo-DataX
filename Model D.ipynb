{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Model D.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"YwcP7zCN1Myq","colab_type":"text"},"source":["# **MODEL D: YOLOv3 + DeepSORT + Early Fused Skeleton + ST-DenseNet** \n","## A unified framework for pedestrian intention prediction.\n","1. **YOLOv3** -> Object detector: responsible to identify and detect objects of interest in a given frame or image.\n","\n","2. **DeepSORT** -> Object Tracker: DeepSORT is responsible tracking the detected objects and enhances the tracking performance by extracting additional features for person re-identification.\n","\n","4. **Early Fused Skeleton** -> Skeleton mapping: Skeletons are then mapped for each tracked pedestrian.\n","\n","3. **Spatio-Temporal DenseNet** -> Classifier: responsible to classify every identified and tracked pedestrian's intention by using the last 16 frames of a pedetrian."]},{"cell_type":"markdown","metadata":{"id":"skyYGOtJLbv-","colab_type":"text"},"source":["*The codes for YOLOv3 was adapted from the GitHub repo: https://github.com/zzh8829/yolov3-tf2*\n","\n","*The codes for DeepSORT was adapted from the GitHub repo: https://github.com/nwojke/deep_sort*\n","\n","*The codes for Skeleton FittingTF-PoseEstimator was adapted from the GitHub repo: https://github.com/ildoonet/tf-pose-estimation*\n","\n","*The codes for ST-DenseNet was adapted from the GitHub repo: https://github.com/GalDude33/DenseNetFCN-3D*\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xylAbzAQLbtr","colab_type":"text"},"source":["## **INSTRUCTIONS TO RUN THE MODEL ON GOOGLE COLAB**\n","\n","This project was completely developed on Google Colab.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"RTcTFn5gWPGb","colab_type":"text"},"source":["###1. Connect runtime to GPU for better/faster results.\n"]},{"cell_type":"markdown","metadata":{"id":"YfiZWEd-PyEb","colab_type":"text"},"source":["###2. Clone the repository to Colab."]},{"cell_type":"code","metadata":{"id":"dR3mfK1PN4vY","colab_type":"code","colab":{}},"source":["# run this to clone the repository Volvo-DataX\n","!git clone https://github.com/mjpramirez/Volvo-DataX"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9GQb7BBdXabJ","colab_type":"text"},"source":["###3. Run this to install dependencies"]},{"cell_type":"code","metadata":{"id":"J7n4Q7G2Ui2i","colab_type":"code","colab":{}},"source":["%cd Volvo-DataX/tf-pose-estimation\n","! pip3 install -r requirements.txt\n","%cd tf_pose/pafprocess\n","! sudo apt install swig\n","!swig -python -c++ pafprocess.i && python3 setup.py build_ext --inplace"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mbmr2WJuVNQX","colab_type":"text"},"source":["###4. Next click this link to activate the folder in your google drive: https://drive.google.com/open?id=1HxKtxBva3US2AJfohlKfjYSdhHvjt2Yc and add a shortcut of the folder to the main drive folder\n","\n","And finally, run the cell to mount your google drive\n"]},{"cell_type":"code","metadata":{"id":"HCqcNrkJ1GcC","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EfFLtD0fWj96","colab_type":"text"},"source":["###5. To run the remaining cells below, observe the comments and run them appropriately. Also running some codes may provide warnings, so please ignore them. "]},{"cell_type":"code","metadata":{"id":"SU6qWoWr1QpP","colab_type":"code","colab":{}},"source":["# run this\n","%cd /content/Volvo-DataX\n","\n","try:\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","import glob\n"," \n","import sys #Run this\n","from absl import app, logging, flags\n","from absl.flags import FLAGS\n","import time\n","import cv2\n","import numpy as np\n","import tensorflow as tf\n","from yolov3_tf2.models import (\n","    YoloV3, YoloV3Tiny\n",")\n","from yolov3_tf2.dataset import transform_images, load_tfrecord_dataset\n","from yolov3_tf2.utils import draw_outputs\n","\n","tf.compat.v1.disable_eager_execution()\n","physical_devices = tf.config.experimental.list_physical_devices('GPU')\n","tf.config.experimental.set_memory_growth(physical_devices[0], True)\n","\n","flags.DEFINE_string('classes', 'data/coco.names', 'path to classes file')\n","flags.DEFINE_string('weights', '/content/drive/My Drive/datax_volvo_additional_files/yolov3_train_5.tf','path to weights file')\n","flags.DEFINE_boolean('tiny', False, 'yolov3 or yolov3-tiny')\n","flags.DEFINE_integer('size', 416, 'resize images to')\n","flags.DEFINE_string('tfrecord', None, 'tfrecord instead of image')\n","flags.DEFINE_integer('num_classes', 1, 'number of classes in the model')\n","flags.DEFINE_string('video', 'data/JAAD_test_video_0339.mp4','path to video file or number for webcam)')\n","flags.DEFINE_string('output','Result_model_D.mp4', 'path to output video')\n","flags.DEFINE_string('output_format', 'mp4v', 'codec used in VideoWriter when saving video to file')\n","\n","app._run_init(['yolov3'], app.parse_flags_with_usage)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DsuZt3UXTH_x","colab_type":"code","colab":{}},"source":["%cd /content/Volvo-DataX/deep_sort\n","from ds_tools.generate_detections import create_box_encoder\n","from ds_application_util import preprocessing\n","from ds_deep_sort import nn_matching\n","from ds_deep_sort.detection import Detection\n","from ds_deep_sort.tracker import Tracker\n","\n","%cd /content/Volvo-DataX/tf-pose-estimation\n","from tf_pose.estimator import TfPoseEstimator\n","from tf_pose.networks import get_graph_path, model_wh\n","from tf_pose.estimator import Human\n","model = TfPoseEstimator(get_graph_path('egen_jaad_1_5'), target_size=(100, 100))\n","\n","%cd /content/Volvo-DataX\n","\n","nms_max_overlap = 1.0\n","max_cosine_distance = 0.2\n","nn_budget = None\n","encoder = create_box_encoder('mars-small128.pb', batch_size=32)\n","metric = nn_matching.NearestNeighborDistanceMetric(\"cosine\", max_cosine_distance, nn_budget)\n","tracker = Tracker(metric)\n","\n","with open('densenet_model.json', 'r') as json_file:\n","    json_savedModel= json_file.read()\n","\n","model_j = tf.keras.models.model_from_json(json_savedModel)\n","model_j.load_weights('densenet_2.hdf5')\n","\n","def pred_func(X_test):\n","  predictions = model_j.predict(X_test[0:1], verbose=0)\n","  Y = np.argmax(predictions[0], axis=0)\n","    \n","  return Y\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-zZrlYUP1Qkr","colab_type":"code","colab":{}},"source":["\n","# Run this\n","FLAGS.yolo_iou_threshold = 0.5\n","FLAGS.yolo_score_threshold = 0.5\n","\n","color = (255, 0, 0) \n","thickness = 2\n","\n","yolo = YoloV3(classes=FLAGS.num_classes)\n","\n","yolo.load_weights(FLAGS.weights).expect_partial()\n","logging.info('weights loaded')\n","\n","class_names = [c.strip() for c in open(FLAGS.classes).readlines()]\n","logging.info('classes loaded')\n","\n","resize_out_ratio = 4.0\n","\n","def run_model():\n","\n","  print('Processing started.......')\n","\n","  fps_time = 0\n","\n","  try:\n","      vid = cv2.VideoCapture(int(FLAGS.video))\n","  except:\n","      vid = cv2.VideoCapture(FLAGS.video)\n","\n","  out = None\n","\n","  if FLAGS.output:\n","      # by default VideoCapture returns float instead of int\n","      width = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH))\n","      height = int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","      fps = int(vid.get(cv2.CAP_PROP_FPS))\n","      codec = cv2.VideoWriter_fourcc(*FLAGS.output_format)\n","      out = cv2.VideoWriter(FLAGS.output, codec, fps, (width, height))\n","\n","  frame = 0\n","  rolling_data={}\n","  fps_time=0\n","\n","  while True:\n","\n","    _, img = vid.read() # reading the image\n","\n","    if img is None:\n","        break\n","        logging.warning(\"Empty Frame\")\n","        time.sleep(0.1)\n","        continue\n","    frame += 1   \n","    currFrame = int(vid.get(cv2.CAP_PROP_POS_FRAMES))\n","    fps = vid.get(cv2.CAP_PROP_FPS)\n","\n","\n","    img_in = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n","    img_orig = np.copy(img)\n","    img_in = tf.expand_dims(img_in, 0)\n","    img_in = transform_images(img_in, FLAGS.size)\n","\n","    t1 = time.time()\n","    boxes, scores, classes, nums = yolo.predict(img_in, steps=1) # yolo\n","    t2 = time.time()\n","    \n","    # for 80 cls yolo\n","\n","    boxes = boxes[:,:nums[0],:].reshape(nums[0], 4)[classes[0][:nums[0]] == 0]\n","    scores = scores[0][:nums[0]][classes[0][:nums[0]] == 0]\n","    nums = len(boxes)\n","\n","    wh = np.flip(img.shape[0:2])\n","    bbtlwh = []\n","    for i in range(nums):\n","\n","\n","      x1y1 = tuple((np.array(boxes[i][0:2]) * wh).astype(np.int32))\n","      x1 = x1y1[0]\n","      y1 = x1y1[1]\n","      x2y2 = tuple((np.array(boxes[i][2:4]) * wh).astype(np.int32))\n","      bbwh = (x2y2[0]-x1y1[0], x2y2[1]-x1y1[1])\n","      w = bbwh[0]\n","      h = bbwh[1]\n","      bbtlwh.append([x1,y1,w,h])\n","\n","    features = encoder(img, bbtlwh) # deepsort input\n","    detections = [Detection(box, conf, feat) for box, conf, feat in zip(bbtlwh, scores, features)] #deep sort output \n","\n","    # Update tracker.\n","    tracker.predict()\n","    tracker.update(detections)\n","    \n","    tracked_bbox = []\n","    ids = []\n","\n","    for track in tracker.tracks:\n","\n","      if not track.is_confirmed() or track.time_since_update > 1:\n","        continue\n","      tracked_bbox.append(track.to_tlwh())\n","      ids.append(track.track_id)\n","\n","\n","    for i in range(len(tracked_bbox)): # densenet \n","\n","      # Show tracker output\n","      x, y, w, h = tracked_bbox[i]\n","      x = int(x)  \n","      y = int(y) \n","      w = int(w) \n","      h = int(h) \n","\n","      # plot the skeletons\n","      try:\n","        cropped = img_orig[y:y + h, x:x + w]\n","        humans = model.inference(cropped, resize_to_default=(w > 0 and h > 0), upsample_size=resize_out_ratio)\n","        humans.sort(key=lambda human: human.score, reverse=True)\n","        skelett = TfPoseEstimator.draw_humans(cropped, humans, imgcopy=True)\n","        img_orig[y:y + h, x:x + w] = skelett\n","        img_orig2 = img_orig\n","      \n","      except:\n","        img_orig2 = img_orig\n","\n","      # looking for previous 16 frames data for a given pedestrian:\n","\n","      intent = 0 #(default, the pedestrian is not crossing)\n","\n","      \n","      if int(ids[i]) in list(rolling_data.keys()):\n","\n","        if len(rolling_data[int(ids[i])]) == 16:\n","          \n","          seq = np.stack(np.array(rolling_data[int(ids[i])]),axis=2)\n","          seq = np.expand_dims(seq, axis=0)\n","          intent = pred_func(seq) # classification output\n","\n","        else:\n","\n","          seq = np.stack(np.array([rolling_data[int(ids[i])][-1]] * 16),axis=2)\n","          seq = np.expand_dims(seq, axis=0)\n","          intent = pred_func(seq) # classification output\n","      \n","\n","      # risky pedestrian identification thru box color\n","\n","      if intent == 1:\n","        color = (0, 0, 255) # Red -> Crossing\n","\n","      else:\n","        color = (0, 255, 0) # green -> Not crossing\n","\n","      fps_time = time.time()\n","      img = cv2.rectangle(img_orig2, (int(x), int(y)), (int(x + w), int(y + h)), color, 2)\n","      img = cv2.putText(img, 'TrackID ' + str(ids[i]), (x, y - 5), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(255, 0, 0), thickness=2)\n","      img = cv2.putText(img,\"Frame No: %d\" % (frame),(10, 10),  cv2.FONT_HERSHEY_SIMPLEX, 0.5,(0, 0, 255), 2)\n","\n","      # storing the data for last 16 frames\n","\n","      try:\n","\n","        if int(ids[i]) in list(rolling_data.keys()): # ID exists in dict\n","\n","          if len(rolling_data[int(ids[i])]) < 16: # bboxes values for 16 frames\n","              \n","            cropped_seq = []\n","            cropped_img = cv2.resize(img_orig[y:h+y, x:w+x],(100,100))\n","            rolling_data[int(ids[i])].append(np.asarray(cropped_img)) # append the image      \n","\n","          else:\n","\n","            del rolling_data[int(ids[i])][0] # delete oldest frame bbox and append latest frame bbox\n","            cropped_seq = []\n","            cropped_img = cv2.resize(img_orig[y:h+y, x:w+x],(100,100))\n","            rolling_data[int(ids[i])].append(np.asarray(cropped_img))\n","              \n","        else:\n","\n","          cropped_seq = []\n","          cropped_img = cv2.resize(img_orig[y:h+y, x:w+x],(100,100))\n","          rolling_data[int(ids[i])] = [np.asarray(cropped_img)]  \n","\n","      except:\n","        pass\n","\n","    \n","    if FLAGS.output:\n","      out.write(img)\n","\n","    if cv2.waitKey(1) == ord('q'):\n","      break\n","\n","  cv2.destroyAllWindows()\n","  print('\\nProcessing completed.......!!!')\n","  print('Check video file in Volvo-DataX folder!')\n","\n","  return"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fw1oCBxzWpPU","colab_type":"text"},"source":["\n","###6. Run this to obtain the Model-D output as a video file named **'Result_model_D.mp4'** in Volvo-DataX folder. \n","After running the run_model() function expect around 15 mins for GPU and 45 mins for CPU"]},{"cell_type":"code","metadata":{"id":"sDixMreeUS_9","colab_type":"code","colab":{}},"source":["run_model()"],"execution_count":0,"outputs":[]}]}